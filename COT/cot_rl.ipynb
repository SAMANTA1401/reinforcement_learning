{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining Chain of Thought (CoT) with Reinforcement Learning (RL) involves enabling an agent to explicitly generate intermediate reasoning steps (CoT) before selecting actions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Components\n",
    "\n",
    "Chain of Thought (CoT) Module: Generates intermediate reasoning steps (e.g., natural language or structured thoughts) based on the environment state.\n",
    "\n",
    "Policy Network: Uses the CoT output to decide actions.\n",
    "\n",
    "RL Training Loop: Trains the agent using rewards, updating both the policy and CoT module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid World Navigation\n",
    "\n",
    "Goal: Train an agent to navigate a grid world to reach a goal. The agent generates CoT steps (e.g., \"Move right to avoid obstacle\") before acting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code implements a Reinforcement Learning (RL) agent using a policy gradient method with a \"Chain of Thought\" (CoT) inspired policy network to navigate a simple grid world environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define CoT + Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9429, -0.9260,  0.1712,  1.3570,  0.6236,  1.4876, -0.3113,\n",
       "           0.3550, -0.1785, -0.1906,  1.0326,  0.1951, -2.4507,  2.7198,\n",
       "          -0.3184, -1.5103,  0.6222,  0.9984, -1.4907,  0.1795, -1.0040,\n",
       "           0.3620, -1.5469, -0.0327,  1.5555]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1, 1, 5*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: The number of batches (or samples). In this case, we're generating a single sample.\n",
    "# 1: The number of channels (or features). In this case, we're generating a single-channel tensor.\n",
    "# 5*5: The number of elements in the tensor. Since 5*5 = 25, we're generating a tensor with 25 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, grid_size=(5, 5)):\n",
    "        self.grid = np.array([\n",
    "            ['S', '.', '.', '.', '.'],\n",
    "            ['.', '#', '.', '.', '.'],\n",
    "            ['.', '.', '#', '.', '.'],\n",
    "            ['.', '.', '.', '#', 'G']\n",
    "        ])\n",
    "        self.grid_size = grid_size\n",
    "        self.reset()\n",
    "        self.action_space = [0, 1, 2, 3]  # Up, Down, Left, Right\n",
    "        self.n_actions = len(self.action_space)\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_pos = self._find_start_pos()\n",
    "        return self._get_state()\n",
    "\n",
    "    def _find_start_pos(self):\n",
    "        for r in range(self.grid_size[0]):\n",
    "            for c in range(self.grid_size[1]):\n",
    "                if self.grid[r, c] == 'S':\n",
    "                    return (r, c)\n",
    "        return (0, 0)  # Default if 'S' not found\n",
    "\n",
    "    def _get_state(self):\n",
    "        # Return a tuple of (grid, agent_position)\n",
    "        return self.grid.copy(), self.agent_pos\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        x, y = self.agent_pos\n",
    "        if action == 0:     # Up\n",
    "            x = max(0, x-1)\n",
    "        elif action == 1:  # Down\n",
    "            x = min(self.grid_size[0] - 1, x+1)\n",
    "        elif action == 2:  # Left\n",
    "            y = max(0, y-1)\n",
    "        elif action == 3:  # Right\n",
    "            y = min(self.grid_size[1] - 1, y+1)\n",
    "\n",
    "        if self.grid[x, y] == '#':\n",
    "            reward = -1    # Hit obstacle\n",
    "        elif self.grid[x, y] == 'G':\n",
    "            reward = 10   # Reach goal\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -0.1  # Step penalty\n",
    "            done = False\n",
    "\n",
    "        self.agent_pos = (x, y)\n",
    "        # print(\"Agent position\", self.agent_pos) # Removed frequent printing\n",
    "        # print(\"state\", self._get_state())\n",
    "        return self._get_state(), reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_dim: The size of the input state representation. In this simplified example, it's assumed to be 25 (5x5 grid flattened)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hidden_dim: The number of hidden units in the LSTM and the subsequent linear layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "action_dim: The number of possible actions (4 in this grid world)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoT_Policy(nn.Module):\n",
    "    def __init__(self, grid_size, hidden_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        # Convolutional layer to process the grid\n",
    "        self.conv = nn.Conv2d(in_channels=4, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # LSTM for temporal processing / \"Chain of Thought\"\n",
    "        self.lstm = nn.LSTM(input_size=16 * grid_size[0] * grid_size[1] + 2,  # Conv output + agent pos\n",
    "                            hidden_size=hidden_dim,\n",
    "                            batch_first=True)\n",
    "\n",
    "        # Policy Network\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "\n",
    "    def _encode_state(self, grid, agent_pos):\n",
    "        # One-hot encode the grid\n",
    "        encoding = np.zeros((4, self.grid_size[0], self.grid_size[1]), dtype=np.float32)\n",
    "        for r in range(self.grid_size[0]):\n",
    "            for c in range(self.grid_size[1]):\n",
    "                if grid[r, c] == 'S':\n",
    "                    encoding[0, r, c] = 1\n",
    "                elif grid[r, c] == '.':\n",
    "                    encoding[1, r, c] = 1\n",
    "                elif grid[r, c] == '#':\n",
    "                    encoding[2, r, c] = 1\n",
    "                elif grid[r, c] == 'G':\n",
    "                    encoding[3, r, c] = 1\n",
    "        return encoding\n",
    "\n",
    "    def forward(self, state_tuple, hidden=None):\n",
    "        grid, agent_pos = state_tuple\n",
    "        encoded_grid = self._encode_state(grid, agent_pos)\n",
    "        grid_tensor = torch.from_numpy(encoded_grid).unsqueeze(0) # Add batch dimension\n",
    "\n",
    "        conv_out = self.relu(self.conv(grid_tensor))\n",
    "        flattened_conv = self.flatten(conv_out)\n",
    "\n",
    "        agent_pos_tensor = torch.tensor(agent_pos, dtype=torch.float32).unsqueeze(0) # Add batch dimension\n",
    "        lstm_input = torch.cat((flattened_conv, agent_pos_tensor), dim=1).unsqueeze(1) # Add sequence dimension\n",
    "\n",
    "        lstm_out, hidden = self.lstm(lstm_input, hidden)\n",
    "        action_logits = self.policy(lstm_out[:, -1, :]) # Take the output of the last time step\n",
    "\n",
    "        return action_logits, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop with Policy Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, model, optimizer, episodes=1000, gamma=0.99):\n",
    "    model.train()\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        hidden_state = None # Initialize LSTM hidden state\n",
    "\n",
    "        while not done:\n",
    "            # Get action probabilities from CoT-guided policy\n",
    "            action_logits, hidden_state = model(state, hidden_state)\n",
    "            action_probs = F.softmax(action_logits, dim=-1)\n",
    "            action_dist = torch.distributions.Categorical(action_probs)\n",
    "            action = action_dist.sample()\n",
    "\n",
    "            # Execute action\n",
    "            next_state, reward, done = env.step(action.item())\n",
    "\n",
    "            # Store log prob and reward\n",
    "            log_probs.append(action_dist.log_prob(action))\n",
    "            rewards.append(reward)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            # Detach hidden state to prevent backpropagation through entire episode\n",
    "            if hidden_state is not None:\n",
    "                hidden_state = (hidden_state[0].detach(), hidden_state[1].detach())\n",
    "\n",
    "        # Calculate discounted returns\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "\n",
    "        # Normalize returns\n",
    "        returns = torch.tensor(returns)\n",
    "        if returns.numel() > 1:\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        else:\n",
    "            returns = torch.zeros_like(returns) # Handle single step episodes\n",
    "\n",
    "        # Policy gradient loss\n",
    "        policy_loss = []\n",
    "        for log_prob, R in zip(log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        # Update model\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Episode {ep+1}, Total Reward: {sum(rewards):.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "softmax() received an invalid combination of arguments - got (tuple, dim=int), but expected one of:\n * (Tensor input, int dim, torch.dtype dtype = None, *, Tensor out = None)\n * (Tensor input, name dim, *, torch.dtype dtype = None)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m      6\u001b[0m episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(env, model, optimizer, episodes)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Get action probabilities from CoT-guided policy\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     action_logits \u001b[38;5;241m=\u001b[39m model(state)\n\u001b[1;32m---> 11\u001b[0m     action_probs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     action_dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(action_probs)\n\u001b[0;32m     13\u001b[0m     action \u001b[38;5;241m=\u001b[39m action_dist\u001b[38;5;241m.\u001b[39msample()\n",
      "\u001b[1;31mTypeError\u001b[0m: softmax() received an invalid combination of arguments - got (tuple, dim=int), but expected one of:\n * (Tensor input, int dim, torch.dtype dtype = None, *, Tensor out = None)\n * (Tensor input, name dim, *, torch.dtype dtype = None)\n"
     ]
    }
   ],
   "source": [
    "grid_size = (4, 5)\n",
    "env = GridWorld(grid_size=grid_size)\n",
    "hidden_dim = 128\n",
    "model = CoT_Policy(grid_size, hidden_dim, env.n_actions)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "episodes = 1000\n",
    "train(env, model, optimizer, episodes=episodes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Evaluation loop\n",
    "model.eval()\n",
    "total_reward = 0\n",
    "num_eval_episodes = 10\n",
    "for _ in range(num_eval_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    hidden_state = None\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            action_logits, hidden_state = model(state, hidden_state)\n",
    "            action_probs = F.softmax(action_logits, dim=-1)\n",
    "            action = torch.argmax(action_probs).item()\n",
    "        state, reward, done = env.step(action)\n",
    "        episode_reward += reward\n",
    "    total_reward += episode_reward\n",
    "\n",
    "print(f\"\\nAverage reward over {num_eval_episodes} evaluation episodes: {total_reward / num_eval_episodes:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Architecture: CoT + Transformer + PPO\n",
    "We’ll use a transformer to generate CoT steps and integrate them with the Proximal Policy Optimization (PPO) algorithm.\n",
    "\n",
    "Key Components\n",
    "CoT Transformer: Generates reasoning steps (e.g., \"Avoid obstacle at (x,y)\").\n",
    "\n",
    "Policy Network: Uses CoT embeddings to select actions.\n",
    "\n",
    "Value Network: Estimates state value for PPO updates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ": Define the CoT Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class CoTTransformer(nn.Module):\n",
    "    def __init__(self, state_dim, cot_dim, n_heads=4):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.TransformerEncoderLayer(\n",
    "            d_model=state_dim, nhead=n_heads, dim_feedforward=256\n",
    "        )\n",
    "        self.cot_proj = nn.Linear(state_dim, cot_dim)  # CoT embeddings\n",
    "\n",
    "    def forward(self, state):\n",
    "        # state: (batch_size, seq_len, state_dim)\n",
    "        cot_embed = self.encoder(state)\n",
    "        cot_steps = self.cot_proj(cot_embed)  # (batch_size, seq_len, cot_dim)\n",
    "        return cot_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Actor-Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, cot_dim, action_dim):\n",
    "        super().__init__()\n",
    "        # Actor (policy)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(cot_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "        # Critic (value)\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(cot_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, cot_steps):\n",
    "        action_logits = self.actor(cot_steps)\n",
    "        value = self.critic(cot_steps)\n",
    "        return action_logits, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPO Training Loop with CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_train(env, cot_model, actor_critic, optimizer, epochs=10, clip_epsilon=0.2):\n",
    "    states, actions, rewards, dones = [], [], [], []\n",
    "    \n",
    "    # Collect trajectories with CoT\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            cot_steps = cot_model(state)\n",
    "            action_logits, value = actor_critic(cot_steps)\n",
    "            action_probs = torch.softmax(action_logits, dim=-1)\n",
    "            action_dist = Categorical(action_probs)\n",
    "            action = action_dist.sample()\n",
    "        \n",
    "        next_state, reward, done = env.step(action.item())\n",
    "        \n",
    "        # Store data\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        state = next_state\n",
    "    \n",
    "    # Compute advantages and returns\n",
    "    returns = compute_returns(rewards, gamma=0.99)\n",
    "    advantages = returns - values\n",
    "    \n",
    "    # PPO loss\n",
    "    for _ in range(epochs):\n",
    "        cot_steps = cot_model(states)\n",
    "        new_action_logits, new_values = actor_critic(cot_steps)\n",
    "        new_action_probs = torch.softmax(new_action_logits, dim=-1)\n",
    "        new_dist = Categorical(new_action_probs)\n",
    "        \n",
    "        # Policy loss\n",
    "        ratio = (new_dist.log_prob(actions) - action_dist.log_prob(actions)).exp()\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1-clip_epsilon, 1+clip_epsilon) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        \n",
    "        # Value loss\n",
    "        value_loss = nn.MSELoss()(new_values, returns)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = policy_loss + 0.5 * value_loss\n",
    "        \n",
    "        # Update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical RL with CoT\n",
    "\n",
    "Decompose tasks into high-level plans (CoT) and low-level actions.\n",
    "\n",
    "Hierarchical Architecture\n",
    "Meta-Controller: Generates CoT plans (e.g., \"Go to Room A → Pick up Key\").\n",
    "\n",
    "Sub-Controller: Executes low-level actions (e.g., \"Move left\", \"Grab\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MetaController.__init__() missing 1 required positional argument: 'plan_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(plan_dim, action_dim)  \u001b[38;5;66;03m# Executes actions\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Training loop:\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m meta_controller \u001b[38;5;241m=\u001b[39m \u001b[43mMetaController\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m sub_controller \u001b[38;5;241m=\u001b[39m SubController(\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Meta-controller generates plan\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: MetaController.__init__() missing 1 required positional argument: 'plan_dim'"
     ]
    }
   ],
   "source": [
    "\n",
    "class MetaController(nn.Module):\n",
    "    def __init__(self, state_dim, plan_dim):\n",
    "        super().__init__()\n",
    "        self.planner = nn.TransformerEncoder(...)  # Generates plans\n",
    "\n",
    "class SubController(nn.Module):\n",
    "    def __init__(self, plan_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.policy = nn.Linear(plan_dim, action_dim)  # Executes actions\n",
    "\n",
    "# Training loop:\n",
    "meta_controller = MetaController(...)\n",
    "sub_controller = SubController(...)\n",
    "\n",
    "# Meta-controller generates plan\n",
    "plan = meta_controller(state)\n",
    "\n",
    "# Sub-controller executes plan\n",
    "for step in plan:\n",
    "    action = sub_controller(step)\n",
    "    env.step(action)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " CoT for Partial Observability\n",
    " \n",
    "Use CoT to maintain memory in partially observable environments (e.g., robot navigation).\n",
    "\n",
    "Memory-Augmented CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoTMemory(nn.Module):\n",
    "    def __init__(self, input_dim, mem_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, mem_dim)  # Track CoT history\n",
    "\n",
    "    def forward(self, state, hidden):\n",
    "        cot_step = generate_cot(state)  # E.g., \"Observed door at (x,y)\"\n",
    "        output, hidden = self.lstm(cot_step, hidden)\n",
    "        return output, hidden"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
